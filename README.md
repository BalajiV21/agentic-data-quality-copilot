
# ğŸš€ **Agentic Data Quality Copilot â€” End-to-End Airflow + PySpark + Agentic AI Pipeline**

### **Modern Data Engineering Pipeline with Airflow â€¢ PySpark â€¢ Delta Bronze Layer â€¢ Agentic AI Alerts (via OpenAI)**

Built by: **Balaji Viswanathan**

---

## ğŸ“Œ **Project Overview**

This project is a fully automated **Agentic Data Quality Pipeline** that fetches data from APIs, performs ETL using **PySpark**, analyzes the pipeline quality using **Agentic AI**, and sends automatic alerts using Airflow.

The pipeline combines:

* **Airflow** (Orchestration)
* **PySpark** (ETL + data processing)
* **Agentic AI (OpenAI GPT-4o-mini)** (Data quality reasoning)
* **Docker** (Containerization)
* **Bronze Data Lake** (Raw â†’ Bronze layer)

The project runs end-to-end every day and generates:

âœ”ï¸ Raw data
âœ”ï¸ Cleaned Bronze layer
âœ”ï¸ ETL logs
âœ”ï¸ Agent Data Quality summary
âœ”ï¸ Email alerts if issues found

---

## ğŸ—ï¸ **Architecture**

```mermaid
flowchart TD
    A[Airflow DAG\n(dq_pipeline)] --> B[API Fetcher\n(fetch_all_data)]
    B --> C[Raw Layer\nJSON files]
    C --> D[PySpark ETL\n(spark_etl.py)]
    D --> E[Bronze Layer]
    E --> F[Agentic AI\n(agent_runner.py)]
    F --> G[Email Alerts]
```

---

## ğŸ“‚ **Project Structure**

```
agentic-data-quality-copilot/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ dq_pipeline_dag.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ api_fetcher.py
â”‚   â”‚       â”œâ”€â”€ spark_etl.py
â”‚   â”‚       â””â”€â”€ agent_runner.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

# âš™ï¸ **Pipeline Steps**

## 1ï¸âƒ£ API Fetcher (Raw Layer)

* Pulls products, users, and orders from a public dummy API
* Saves into:

```
/opt/airflow/data/raw/products.json
/opt/airflow/data/raw/users.json
/opt/airflow/data/raw/orders.json
```

---

## 2ï¸âƒ£ PySpark ETL (Bronze Layer)

* Reads the raw data
* Normalizes nested JSON
* Selects required fields
* Writes Bronze layer:

```
/opt/airflow/data/bronze/products/
opt/airflow/data/bronze/users/
opt/airflow/data/bronze/orders/
```

---

## 3ï¸âƒ£ Agentic AI (Data Quality Reasoning)

The AI:

* Reads ETL logs
* Detects missing values
* Detects abnormal counts
* Generates human-friendly summary
* Add final tag:

```
ISSUES_FOUND=True or False
```

Output saved to:

```
/opt/airflow/data/agent_summary.txt
```

---

## 4ï¸âƒ£ Airflow Email Alerts

If the AI finds issues â†’ email is sent.

---

# ğŸ³ **Run the Entire Project Using Docker**

### **Start Services**

```
docker compose up -d
```

### **Access Airflow**

```
http://localhost:8080
```

**username:** airflow
**password:** airflow

---

# â–¶ï¸ **Trigger the DAG**

In Airflow UI â†’ Trigger â€œdq_pipelineâ€

This runs:

1. API Fetch
2. Spark ETL
3. Agentic AI
4. Email

---

# ğŸ“‘ **Agent Summary Example**

Example output:

```
Products seem complete.
Users list has 2 records missing email.
Orders contain invalid quantity values.

ISSUES_FOUND=True
```

---

# ğŸ“§ **Automatic Email Example**

```
Subject: Data Quality Issue Detected

Hi Team,
The pipeline detected data quality issues today.

- 2 user records missing email
- 1 product has negative stock

ISSUES_FOUND=True
```

---

# ğŸ§  **Tech Used**

| Component     | Technology             |
| ------------- | ---------------------- |
| Orchestration | Airflow (Docker)       |
| ETL Engine    | Apache Spark / PySpark |
| Data Layer    | Raw + Bronze           |
| AI            | OpenAI GPT-4o-mini     |
| Orchestration | Airflow DAG            |
| Environment   | Docker Compose         |
| Language      | Python 3               |

---

# ğŸ **End-to-End Pipeline Output**

After a successful run, you will see:

âœ”ï¸ Raw files
âœ”ï¸ Bronze data
âœ”ï¸ ETL logs
âœ”ï¸ AI summary
âœ”ï¸ Email alerts (if needed)

